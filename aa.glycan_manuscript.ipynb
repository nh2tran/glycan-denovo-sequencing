{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import itertools\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"tf.__version__ =\", tf.__version__)\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from spektral.layers import GCNConv, GlobalSumPool\n",
    "from spektral.data import Dataset\n",
    "from spektral.data import Graph\n",
    "from spektral.data import BatchLoader\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')\n",
    "\n",
    "import glypy\n",
    "from glypy.io.nomenclature import synonyms, identity\n",
    "from glypy.plot import plot as glypy_plot\n",
    "from glypy.io import glycoct as glypy_glycoct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# I/O FUNCTIONS and DATA PRE-PROCESSING\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     24,
     27,
     28,
     64,
     65,
     69,
     78,
     93,
     135,
     137,
     143,
     154,
     161,
     165
    ]
   },
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "\n",
    "# input data folder includes the following main files:\n",
    "#   \"fraction_[1-5].mgf\": MS2 spectra, exported from PEAKS GlycanFinder\n",
    "#   \"fraction_[1-5].labeled.csv\": glycoPSM from PEAKS GlycanFinder database search\n",
    "#   \"glycans.txt\": glycan database\n",
    "\n",
    "# data_folder = \"data.training/glycan/mouse_brain/\"\n",
    "# num_fractions = 5\n",
    "# mode = 'training'\n",
    "data_folder = \"data.training/glycan/Demo_IgG_Orbitrap/\"\n",
    "num_fractions = 3\n",
    "mode = 'evaluation'\n",
    "print(\"data_folder =\", data_folder)\n",
    "print(\"num_fractions =\", num_fractions)\n",
    "print(\"mode =\", mode)\n",
    "print()\n",
    "\n",
    "fraction_id_list = list(range(1, 1 + num_fractions))\n",
    "\n",
    "# read mgf files\n",
    "mgf_files = [data_folder + 'fraction_' + str(x) + '.mgf' for x in fraction_id_list]\n",
    "input_spectrum_file = data_folder + \"spectrum.mgf\"\n",
    "print(\"Prepare input_spectrum_file =\", input_spectrum_file)\n",
    "if os.path.exists(input_spectrum_file):\n",
    "    print(\"input_spectrum_file exists!\")\n",
    "    print()\n",
    "else:\n",
    "    def merge_mgf_file(input_file_list, fraction_list, output_file):\n",
    "        \"\"\"Merge multiple mgf files into one, adding fraction ID to scan ID.\n",
    "\n",
    "            Usage:\n",
    "                folder_path = \"data.training/aa.hla.bassani.nature_2016.mel_16.class_1/\"\n",
    "                fraction_list = range(0, 10+1)\n",
    "                merge_mgf_file(\n",
    "                    input_file_list=[folder_path + \"export_\" + str(i) + \".mgf\" for i in fraction_list],\n",
    "                    fraction_list=fraction_list,\n",
    "                    output_file=folder_path + \"spectrum.mgf\")\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"merge_mgf_file()\")\n",
    "\n",
    "        # iterate over mgf files and their lines\n",
    "        counter = 0\n",
    "        with open(output_file, mode=\"w\") as output_handle:\n",
    "            for input_file, fraction in zip(input_file_list, fraction_list):\n",
    "                print(\"input_file = \", os.path.join(input_file))\n",
    "                with open(input_file, mode=\"r\") as input_handle:\n",
    "                    for line in input_handle:\n",
    "                        if \"SCANS=\" in line: # a spectrum found\n",
    "                            counter += 1\n",
    "                            scan = re.split('=|\\n|\\r', line)[1]\n",
    "                            # re-number scan id\n",
    "                            output_handle.write(\"SCANS=F{0}:{1}\\n\".format(fraction, scan))\n",
    "                        else:\n",
    "                            output_handle.write(line)\n",
    "        print(\"output_file = {0:s}\".format(output_file))\n",
    "        print(\"counter = {0:d}\".format(counter))\n",
    "        print()\n",
    "    merge_mgf_file(mgf_files, fraction_id_list, input_spectrum_file)\n",
    "print()\n",
    "\n",
    "# store spectrum_location_dict to quickly retrieve spectrum from its scan id (copied from deepnovo_worker_io:get_location)\n",
    "spectrum_location_file = input_spectrum_file + '.locations.pkl'\n",
    "if os.path.exists(spectrum_location_file):\n",
    "    with open(spectrum_location_file, 'rb') as fr:\n",
    "        print(\"WorkerIO: read cached spectrum locations\")\n",
    "        data = pickle.load(fr)\n",
    "        spectrum_location_dict, spectrum_rtinseconds_dict, spectrum_count = data\n",
    "else:\n",
    "    print(\"WorkerIO: build spectrum location from scratch\")\n",
    "    input_spectrum_handle = open(input_spectrum_file, 'r')\n",
    "    spectrum_location_dict = {}\n",
    "    spectrum_rtinseconds_dict = {}\n",
    "    line = True\n",
    "    while line:\n",
    "        current_location = input_spectrum_handle.tell()\n",
    "        line = input_spectrum_handle.readline()\n",
    "        if \"BEGIN IONS\" in line:\n",
    "            spectrum_location = current_location\n",
    "        elif \"SCANS=\" in line:\n",
    "            scan = re.split('=|\\r|\\n', line)[1]\n",
    "            spectrum_location_dict[scan] = spectrum_location\n",
    "        elif \"RTINSECONDS=\" in line:\n",
    "            rtinseconds = float(re.split('=|\\r|\\n', line)[1])\n",
    "            spectrum_rtinseconds_dict[scan] = rtinseconds\n",
    "    spectrum_count = len(spectrum_location_dict)\n",
    "    with open(spectrum_location_file, 'wb') as fw:\n",
    "        pickle.dump((spectrum_location_dict, spectrum_rtinseconds_dict, spectrum_count), fw)\n",
    "    input_spectrum_handle.close()\n",
    "print(\"len(spectrum_location_dict) =\", len(spectrum_location_dict))\n",
    "print()\n",
    "# funtion to retrieve spectrum from its scan id\n",
    "def get_spectrum(input_spectrum_handle, spectrum_location_dict, scan):\n",
    "\n",
    "    spectrum_location = spectrum_location_dict[scan]\n",
    "    input_file_handle = input_spectrum_handle\n",
    "    input_file_handle.seek(spectrum_location)\n",
    "\n",
    "    # parse header lines\n",
    "    line = input_file_handle.readline()\n",
    "    assert \"BEGIN IONS\" in line, \"Error: wrong input BEGIN IONS\"\n",
    "#     line = input_file_handle.readline()\n",
    "#     assert \"TITLE=\" in line, \"Error: wrong input TITLE=\"\n",
    "#     line = input_file_handle.readline()\n",
    "#     assert \"PEPMASS=\" in line, \"Error: wrong input PEPMASS=\"\n",
    "#     line = input_file_handle.readline()\n",
    "#     assert \"CHARGE=\" in line, \"Error: wrong input CHARGE=\"\n",
    "#     line = input_file_handle.readline()\n",
    "#     assert \"SCANS=\" in line, \"Error: wrong input SCANS=\"\n",
    "#     line = input_file_handle.readline()\n",
    "#     assert \"RTINSECONDS=\" in line, \"Error: wrong input RTINSECONDS=\"\n",
    "    while not \"RTINSECONDS=\" in line:\n",
    "        line = input_file_handle.readline()\n",
    "\n",
    "    # parse fragment ions\n",
    "    mz_list = []\n",
    "    intensity_list = []\n",
    "    line = input_file_handle.readline()\n",
    "    while not \"END IONS\" in line:\n",
    "        mz, intensity = re.split(' |\\n', line)[:2]\n",
    "        mz_float = float(mz)\n",
    "        intensity_float = float(intensity)\n",
    "        # skip an ion if its mass > MZ_MAX\n",
    "#         if mz_float > 3000:\n",
    "#             line = input_file_handle.readline()\n",
    "#             continue\n",
    "        mz_list.append(mz_float)\n",
    "        intensity_list.append(intensity_float)\n",
    "        line = input_file_handle.readline()\n",
    "\n",
    "    return mz_list, intensity_list\n",
    "\n",
    "\n",
    "# read csv files\n",
    "if mode == 'prediction':\n",
    "    csv_files = [data_folder + 'fraction_' + str(i) + '.csv' for i in fraction_id_list]\n",
    "elif mode == 'evaluation' or mode == 'training':\n",
    "    csv_files = [data_folder + 'fraction_' + str(i) + '.labeled.csv' for i in fraction_id_list]\n",
    "    glycan_score_cutoff = 1.\n",
    "    print(\"glycan_score_cutoff =\", glycan_score_cutoff)\n",
    "print(\"Prepare csv_files =\", csv_files)\n",
    "glycan_psm = {x:[] for x in fraction_id_list}\n",
    "for fraction_id, csvfile in zip(fraction_id_list, csv_files):\n",
    "    with open(csvfile, 'r') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            if mode == 'evaluation' or mode == 'training':\n",
    "                glycan_score = float(row['Glycan Score'])\n",
    "                if glycan_score < glycan_score_cutoff:\n",
    "                    continue\n",
    "            row['fraction_id'] = fraction_id\n",
    "            glycan_psm[fraction_id].append(row)\n",
    "total_psm = 0\n",
    "for fraction_id, psm_list in glycan_psm.items():\n",
    "    print(\"fraction_id =\", fraction_id, \",\", \"len(psm_list) =\", len(psm_list))\n",
    "    total_psm += len(psm_list)\n",
    "print(\"total_psm =\", total_psm)\n",
    "print()\n",
    "\n",
    "# for evaluation for training, read glycan database\n",
    "if mode == 'evaluation' or mode == 'training':\n",
    "    glycan_db_file = data_folder + 'glycans.txt'\n",
    "    print(\"Prepare glycan_db_file =\", glycan_db_file)\n",
    "    glycan_dict = {}\n",
    "    with open(glycan_db_file, 'r') as handle:\n",
    "        text = handle.read().strip()\n",
    "        text = text.split('GLYCAN END')[:-1]\n",
    "        text = [x.strip() for x in text]\n",
    "        for block in text:\n",
    "            lines = block.split('\\n')\n",
    "            glycan = {}\n",
    "            res_lin = []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if 'GLYCAN' not in line:\n",
    "                    res_lin.append(line)\n",
    "                elif '=' in line:\n",
    "                    k, v = line.split('=')\n",
    "                    glycan[k] = v\n",
    "            res_lin = '\\n'.join(res_lin)\n",
    "            glycan['GLYCAN'] = glypy_glycoct.loads(res_lin)\n",
    "            glycan_dict[glycan['GLYCANID']] = glycan\n",
    "    print(\"len(glycan_dict) =\", len(glycan_dict))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     4,
     9,
     32,
     37
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the sugar classes for the classification task\n",
    "\n",
    "# they can be loaded from the file \"sugar_classes.pkl\"\n",
    "sugar_classes_file = \"sugar_classes.pkl\"\n",
    "if os.path.exists(sugar_classes_file):\n",
    "    with open(sugar_classes_file, 'rb') as fr:\n",
    "        print(\"Load sugar classes  from\", sugar_classes_file)\n",
    "        sugar_classes = pickle.load(fr)\n",
    "# or they can be derived from the glycan_psm of the training data\n",
    "elif mode == 'training':\n",
    "    print(\"Collect sugar classes from glycan_psm\")\n",
    "    sugar_name_set = set()\n",
    "    for fraction_id, psm_list in glycan_psm.items():\n",
    "        for psm in psm_list:\n",
    "            # read sugar names\n",
    "            # many glycoct have the same shape and mass, but different colors\n",
    "            # we can use sugar names (like super-classes) to reduce the number of classes\n",
    "            sugar_name_list = re.split('\\(|\\)', psm['Glycan'])\n",
    "            sugar_name_list = [x for x in sugar_name_list if x and not(x.isdigit())]\n",
    "            sugar_name_set.update(set(sugar_name_list))\n",
    "    # make sure that all sugar names exist in glypy\n",
    "    #sugar_name_set.remove('HexA')\n",
    "    #sugar_name_set.add('GlcA')\n",
    "    #sugar_name_set.add('KDN')\n",
    "    #sugar_name_set.add('Xyl')\n",
    "    assert all([x in glypy.monosaccharides for x in sugar_name_set])\n",
    "    sugar_classes = sorted(list(sugar_name_set))\n",
    "    with open(sugar_classes_file, 'wb') as fw:\n",
    "        pickle.dump(sugar_classes, fw)\n",
    "num_sugars = len(sugar_classes)\n",
    "print(\"sugar_classes = \", sugar_classes)\n",
    "print(\"num_sugars = \", num_sugars)\n",
    "for name in sugar_classes:\n",
    "        print(name, glypy.monosaccharides[name].mass(), sep='\\t')\n",
    "print()\n",
    "\n",
    "# function to retrieve the class of a sugar node in a glycan tree based on its mass\n",
    "def get_class_name(node):\n",
    "    \n",
    "    sugar_mass_tolerance = 0.05\n",
    "    node_mass = node.mass()\n",
    "    node_name = ''\n",
    "    for name in sugar_classes:\n",
    "        if abs(node_mass - glypy.monosaccharides[name].mass()) < sugar_mass_tolerance:\n",
    "            node_name = name\n",
    "            break\n",
    "    if not node_name:\n",
    "        print(\"Unknown sugar: {}\".format(node))\n",
    "        print(node_mass)\n",
    "        print(stop)\n",
    "    return node_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Clone n_link_core & n_link_core_fuc of glycan trees\n",
    "\n",
    "# n_link_core\n",
    "n_link_core = glypy.glycans['N-Linked Core'].clone()\n",
    "glypy_plot(n_link_core, label=True)\n",
    "print(\"len(n_link_core.index) =\", len(n_link_core.index))\n",
    "print()\n",
    "\n",
    "# n_link_core_fuc\n",
    "sugar_fuc = glypy.monosaccharides['Fuc'].clone()\n",
    "n_link_core_fuc = n_link_core.clone()\n",
    "n_link_core_fuc.root.add_monosaccharide(sugar_fuc, position=6, child_position=1)\n",
    "n_link_core_fuc.reindex(method='bfs')\n",
    "glypy_plot(n_link_core_fuc, label=True)\n",
    "print(\"len(n_link_core_fuc.index) =\", len(n_link_core_fuc.index))\n",
    "for leaf in n_link_core_fuc.index:\n",
    "    print(leaf)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     20,
     67
    ]
   },
   "outputs": [],
   "source": [
    "# Essential functions to calculate B & Y ions, glycopsm, and glycan comparison\n",
    "# only use y-ions ???\n",
    "\n",
    "def get_b_y_set(glycan, resolution):\n",
    "\n",
    "    mass_free_reducing_end = 18.0105546\n",
    "    glycan_clone = glycan.clone()\n",
    "    glycan_b_set = set()\n",
    "    glycan_y_set = set()\n",
    "    for links, frags in itertools.groupby(glycan_clone.fragments(), lambda f: f.link_ids.keys()):\n",
    "        y_ion, b_ion = list(frags)\n",
    "        y_mass_reduced = y_ion.mass - mass_free_reducing_end\n",
    "        b_mass_int = int(round(b_ion.mass * resolution))\n",
    "        y_mass_int = int(round(y_mass_reduced * resolution))\n",
    "        glycan_b_set.add(b_mass_int)\n",
    "        glycan_y_set.add(y_mass_int)\n",
    "    \n",
    "    return glycan_b_set, glycan_y_set\n",
    "\n",
    "\n",
    "def compute_glycopsm_score(glycan, peptide_only_mass, mz1_list, intensity_list):\n",
    "\n",
    "    # calculate theoretical b, y ions of the glycan\n",
    "    resolution = 1e3\n",
    "    glycan_b_set, glycan_y_set = get_b_y_set(glycan, resolution)\n",
    "    glycan_b_list = sorted(list(glycan_b_set))\n",
    "    glycan_y_list = sorted(list(glycan_y_set))\n",
    "    glycopeptide_b_list = [float(x)/resolution for x in glycan_b_list]\n",
    "    glycopeptide_y_list = [peptide_only_mass + float(x)/resolution for x in glycan_y_list]\n",
    "    # only use y-ions\n",
    "    glycopeptide_ion_list = glycopeptide_y_list\n",
    "    num_glyco = len(glycopeptide_ion_list)\n",
    "\n",
    "    # calculate the neutral masses and normalize the intensities of fragment ions in the spectrum\n",
    "    charge = 1.0\n",
    "    mass_H = 1.0078\n",
    "    mz0_list = [mz1 - charge*mass_H for mz1 in mz1_list]\n",
    "    num_mz0 = len(mz0_list)\n",
    "    intensity_max = max(intensity_list)\n",
    "    intensity_list = [x/intensity_max for x in intensity_list]\n",
    "\n",
    "    # convert and broadcast to np arrays of shape (num_mz0, num_glyco)\n",
    "    glycopeptide_array = np.array(glycopeptide_ion_list)\n",
    "    glycopeptide_array = np.broadcast_to(glycopeptide_array, shape=(num_mz0, num_glyco))\n",
    "    mz0_array = np.array(mz0_list)\n",
    "    mz0_array = np.broadcast_to(mz0_array, shape=(num_glyco, num_mz0))\n",
    "    mz0_array = np.transpose(mz0_array)\n",
    "    intensity_array = np.array(intensity_list)\n",
    "    intensity_array = np.broadcast_to(intensity_array, shape=(num_glyco, num_mz0))\n",
    "    intensity_array = np.transpose(intensity_array)\n",
    "\n",
    "    # calculate glycopsm as following:\n",
    "    # intensity is weighted by mass error sigma (similar to Rui's PointNovo paper)\n",
    "    # softmax along the mz dimension is used to select the fragment ion closest to the theoretical ion\n",
    "    # add penalty if best peak has intensity < 0.5%\n",
    "    C_const = 10.0\n",
    "    delta = np.abs(mz0_array - glycopeptide_array)\n",
    "    delta_C = -delta * C_const\n",
    "    sigma = np.exp(delta_C)\n",
    "    sigma_softmax = softmax(delta_C, axis=0)\n",
    "    glycopsm = np.sum(sigma_softmax * sigma * intensity_array, axis=0)\n",
    "    glycopsm = np.log((glycopsm+0.0001)/0.005)\n",
    "    glycopsm_score = np.sum(glycopsm)\n",
    "    \n",
    "    return glycan_y_list, glycopsm\n",
    "\n",
    "\n",
    "def test_glycan_accuracy(target_glycans, predict_glycans, top=1):\n",
    "    \n",
    "    print(\"test_glycan_accuracy()\")\n",
    "    \n",
    "    resolution = 1e3\n",
    "    num_targets = float(len(target_glycans))\n",
    "    num_predicts = float(len([x for x in predict_glycans if x]))\n",
    "    num_target_y = 0.\n",
    "    num_predict_y = 0.\n",
    "    num_correct_y = 0.\n",
    "    num_correct_glycans = 0.\n",
    "    with open('test_glycan_accuracy.csv', 'w') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "        csvwriter.writerow(['best_predict_y', 'best_correct_y', 'best_score'])\n",
    "        for target, predict in zip(target_glycans, predict_glycans):\n",
    "            target_b_set, target_y_set = get_b_y_set(target, resolution)\n",
    "            num_target_y += len(target_y_set)\n",
    "            best_predict_y = 0.\n",
    "            best_correct_y = 0.\n",
    "            best_correct_glycan = 0.\n",
    "            best_score = None\n",
    "            for candidate, score in predict[:top]:\n",
    "                predict_b_set, predict_y_set = get_b_y_set(candidate, resolution) if candidate else (set(), set())\n",
    "                correct_y_set = target_y_set.intersection(predict_y_set)\n",
    "                correct_glycan = 1 if target_y_set == predict_y_set else 0\n",
    "                if len(correct_y_set) > best_correct_y:\n",
    "                    best_predict_y = len(predict_y_set)\n",
    "                    best_correct_y = len(correct_y_set)\n",
    "                    best_correct_glycan = correct_glycan\n",
    "                    best_score = score\n",
    "\n",
    "#                 if correct_glycan == 0:\n",
    "#                     fig, axes = pyplot.subplots(1, 2)\n",
    "#                     fig.set_size_inches(12, 4)\n",
    "#                     glypy_plot(target, ax=axes[0], center=True)\n",
    "#                     glypy_plot(candidate, ax=axes[1], center=True)\n",
    "#                     print(stop)\n",
    "#             if not predict:\n",
    "#                 glypy_plot(target, center=True)\n",
    "#                 print(stop)\n",
    "\n",
    "            csvwriter.writerow([best_predict_y, best_correct_y, best_score])\n",
    "            num_predict_y += best_predict_y\n",
    "            num_correct_y += best_correct_y\n",
    "            num_correct_glycans += best_correct_glycan\n",
    "    \n",
    "    sensitivity_y = num_correct_y / num_target_y\n",
    "    sensitivity_glycan = num_correct_glycans / num_targets\n",
    "    precision_y = num_correct_y / num_predict_y\n",
    "    \n",
    "    print(\"num_targets = \", num_targets)\n",
    "    print(\"num_predicts = \", num_predicts)\n",
    "    print(\"num_correct_glycans = \", num_correct_glycans)\n",
    "    print(\"sensitivity_glycan = {:.2f}\".format(sensitivity_glycan))\n",
    "    print(\"num_target_y = \", num_target_y)\n",
    "    print(\"num_predict_y = \", num_predict_y)\n",
    "    print(\"num_correct_y = \", num_correct_y)\n",
    "    print(\"sensitivity_y = {:.2f}\".format(sensitivity_y))\n",
    "    print(\"precision_y = {:.2f}\".format(precision_y))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# MODEL PREPARATION\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2,
     4
    ]
   },
   "outputs": [],
   "source": [
    "# Define Graph Neural Network model\n",
    "\n",
    "class GnnModel(Model):\n",
    "\n",
    "    def __init__(self, n_hidden, version):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.version = version\n",
    "        self.graph_conv = GCNConv(n_hidden, name='gcn_conv', kernel_regularizer=regularizers.l2(0.01))\n",
    "        #self.dropout = Dropout(0.5, name='dropout')\n",
    "        self.pool = GlobalSumPool(name='global_sum_pool')\n",
    "        #self.dense_1 = Dense(n_hidden, name='dense_1', kernel_regularizer=regularizers.l2(0.01))\n",
    "        self.dense_last = Dense(num_sugars,\n",
    "                                kernel_regularizer=regularizers.l2(0.01),\n",
    "#                                 activation='softmax',\n",
    "                                name='dense_last')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        # glycan_y_superset model\n",
    "        outputs.append(inputs[0])\n",
    "\n",
    "        # 8 separate GNN models on 8 inputs\n",
    "        for input_ in inputs[1:]:\n",
    "            out = self.graph_conv(input_)\n",
    "            #out = self.dropout(out)\n",
    "            out = self.pool(out)\n",
    "            #out = self.dense_1(out)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        # only use glycan_y_superset model or combine two models\n",
    "        if self.version == 'linear':\n",
    "            out = outputs[0]\n",
    "        elif self.version == 'gnn':\n",
    "            out = tf.concat(outputs, axis=1)\n",
    "\n",
    "        # last logit layer\n",
    "        out = self.dense_last(out)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2,
     33
    ]
   },
   "outputs": [],
   "source": [
    "# Function to convert glycan trees to graphs\n",
    "\n",
    "def tree_to_graph(tree):\n",
    "\n",
    "    nodes = []\n",
    "    node_id_to_index ={}\n",
    "    for node in tree.clone().index[::-1]:\n",
    "        node_index = len(nodes)\n",
    "        node_id = node.id\n",
    "        parents = node.parents()\n",
    "        if parents:\n",
    "            node.drop_monosaccharide(parents[0][0])\n",
    "        node_name = get_class_name(node)\n",
    "        node_sugar_index = sugar_classes.index(node_name)\n",
    "        nodes.append({'index': node_index, 'id': node_id, 'name': node_name, 'sugar_index': node_sugar_index})\n",
    "        node_id_to_index[node_id] = node_index\n",
    "    num_nodes = len(nodes)\n",
    "    \n",
    "    num_nodes_max = max(20, num_nodes)\n",
    "    nodes_onehot = np.zeros((num_nodes_max, num_sugars)) # np.zeros((num_nodes, num_sugars))\n",
    "    nodes_onehot[np.arange(num_nodes), np.array([node['sugar_index'] for node in nodes])] = 1\n",
    "    # padding 'Hex' up to 20 nodes\n",
    "    if num_nodes < num_nodes_max:\n",
    "        nodes_onehot[num_nodes:num_nodes_max, sugar_classes.index('Hex')] = 1\n",
    "    \n",
    "    adjacency_matrix = np.zeros((num_nodes_max, num_nodes_max)) # np.zeros((num_nodes, num_nodes))\n",
    "    for link in tree.link_index:\n",
    "        parent_index = node_id_to_index[link.parent.id]\n",
    "        child_index = node_id_to_index[link.child.id]\n",
    "        adjacency_matrix[parent_index, child_index] = 1\n",
    "    \n",
    "    return nodes_onehot, adjacency_matrix\n",
    "\n",
    "class Trees_to_Graphs(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, trees_labels, pseudo_graph, **kwargs):\n",
    "        self.trees_labels = trees_labels\n",
    "        self.pseudo_graph = pseudo_graph\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    # The `download()` method is automatically called if the path returned by\n",
    "    # `Dataset.path` does not exists (default `~/.spektral/datasets/ClassName/`).\n",
    "    def download(self):\n",
    "        data = ...  # Download from somewhere\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        output = []\n",
    "        if self.pseudo_graph:\n",
    "            output.append(self.pseudo_graph)\n",
    "        for tree, label in self.trees_labels:\n",
    "            graph_x, graph_a = tree_to_graph(tree)\n",
    "            graph = Graph(x=graph_x, a=graph_a, y=label)\n",
    "            output.append(graph)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     32,
     61
    ]
   },
   "outputs": [],
   "source": [
    "# Functions to prepare training/testing data\n",
    "\n",
    "# prepare training and testing samples from the input data\n",
    "def prepare_training_samples(input_spectrum_file, glycan_psm, fraction_id_list):\n",
    "\n",
    "    input_spectrum_handle = open(input_spectrum_file, 'r')\n",
    "    sample_list = []\n",
    "    for fraction_id in fraction_id_list:\n",
    "        psm_list = glycan_psm[fraction_id]\n",
    "        \n",
    "        print(\"fraction_id = {0:d}, len(psm_list) = {1:d}\".format(fraction_id, len(psm_list)))\n",
    "        for index, psm in enumerate(psm_list):\n",
    "\n",
    "            if ((index+1) % 1000 == 0):\n",
    "                print(\"Processed {:d} PSMs...\".format(index+1))\n",
    "\n",
    "            # read peptide and glycan\n",
    "            peptide_mass = float(psm['Mass'])\n",
    "            target_glycan_id = psm['Glycan ID']\n",
    "            target_glycan_mass = float(psm['Glycan Mass'])\n",
    "            peptide_only_mass = peptide_mass - target_glycan_mass\n",
    "            #target_glycan = glycan_dict[target_glycan_id]['GLYCAN'].clone()#index_method='bfs')\n",
    "            target_glycan_2idx = [glycan_dict[target_glycan_id]['GLYCAN'].clone(),\n",
    "                                  glycan_dict[target_glycan_id]['GLYCAN'].clone(index_method='bfs')]\n",
    "\n",
    "            # read spectrum\n",
    "            scan = 'F' + str(fraction_id) + ':' + psm['Scan']\n",
    "            mz1_list, intensity_list = get_spectrum(input_spectrum_handle, spectrum_location_dict, scan)\n",
    "\n",
    "            # recursively partition the glycan tree into tree_glycopsm_list\n",
    "            tree_glycopsm_list = []\n",
    "            for target_glycan in target_glycan_2idx:\n",
    "                for leaf in target_glycan.index[::-1]:\n",
    "                    parents = leaf.parents()\n",
    "                    if parents:\n",
    "                        leaf.drop_monosaccharide(parents[0][0])\n",
    "                        leaf_name = get_class_name(leaf)\n",
    "                        #subtree = target_glycan.clone()\n",
    "                        candidate_glycopsm = []\n",
    "                        candidate_tree = []\n",
    "                        for name in sugar_classes:\n",
    "                            sugar = glypy.monosaccharides[name].clone()\n",
    "                            parents[0][1].add_monosaccharide(sugar)\n",
    "                            glycan_y_list, glycopsm = compute_glycopsm_score(target_glycan, peptide_only_mass, mz1_list, intensity_list)\n",
    "                            candidate_glycopsm.append((glycan_y_list, glycopsm))\n",
    "                            candidate_tree.append(target_glycan.clone())\n",
    "                            sugar.drop_monosaccharide(-1)\n",
    "                        tree_glycopsm_list.append((leaf_name, candidate_glycopsm, candidate_tree))\n",
    "\n",
    "#                         print(\"leaf_name = \", leaf_name)\n",
    "#                         glypy_plot(subtree, center=True, label=True)\n",
    "#                         print(candidate_glycopsm)\n",
    "#                         print(stop)\n",
    "\n",
    "            sample_list.append(tree_glycopsm_list)\n",
    "    input_spectrum_handle.close()\n",
    "    \n",
    "    return sample_list\n",
    "\n",
    "\n",
    "# convert leaf, glycopsm, trees of training/testing samples into np arrays\n",
    "def prepare_np_arrays(sample_list, glycan_y_superset):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    x_len = len(glycan_y_superset)\n",
    "    x_trees_labels = []\n",
    "    for sample in sample_list:\n",
    "        for leaf_name, candidate_glycopsm, candidate_tree in sample:\n",
    "            x_subarray = np.zeros((num_sugars , x_len))\n",
    "            for candidate, (glycan_y_list, glycopsm) in enumerate(candidate_glycopsm):\n",
    "                for y, score in zip(glycan_y_list, glycopsm):\n",
    "                    if y in glycan_y_superset:\n",
    "                        idx = glycan_y_superset.index(y)\n",
    "                        x_subarray[candidate, idx] = score\n",
    "            x_array.append(x_subarray)\n",
    "            y_array.append(sugar_classes.index(leaf_name))\n",
    "\n",
    "            for tree in candidate_tree:\n",
    "                #glypy_plot(tree, center=True, label=True)\n",
    "                label = 0\n",
    "                x_trees_labels.append((tree, label))\n",
    "                \n",
    "    x_array = np.array(x_array) # shape (num_samples*num_leaves, num_sugars, x_len)\n",
    "    x_array = np.reshape(x_array, (-1, num_sugars *x_len)) # shape (num_samples*num_leaves, num_sugars*x_len)\n",
    "    y_array = np.array(y_array) # shape (num_samples*num_leaves,)\n",
    "    \n",
    "    x_graphs = Trees_to_Graphs(x_trees_labels, pseudo_graph=None) # shape (num_samples*num_leaves*num_sugars,)\n",
    "    # use BatchLoader to do zero padding for all graphs to have the same size\n",
    "    # put all graphs into 1 batch\n",
    "    loader = BatchLoader(x_graphs, batch_size=len(x_graphs), shuffle=False)\n",
    "    x_n_array = []\n",
    "    x_a_array = []\n",
    "    for batch in loader.load():\n",
    "        # each batch is a tuple (inputs, labels)\n",
    "        # inputs is a tuple containing:\n",
    "        # 0: node attributes of shape [batch, n_max, n_node_features];\n",
    "        # 1: adjacency matrices of shape [batch, n_max, n_max];\n",
    "        x_n_array.append(batch[0][0]) # node features\n",
    "        x_a_array.append(batch[0][1]) # adjacency matrix\n",
    "        # because batch_size=len(x_graphs), one iteration is enough\n",
    "        break\n",
    "    x_n_array = np.array(x_n_array) # shape (1, num_samples*num_leaves*num_sugars, n_max, n_node_features)\n",
    "    x_a_array = np.array(x_a_array) # shape (1, num_samples*num_leaves*num_sugars, n_max, n_max)\n",
    "    # reshape to (num_samples*num_leaves, num_sugars, , )\n",
    "    n_shape = list(x_n_array.shape)\n",
    "    a_shape = list(x_a_array.shape)\n",
    "    n_shape[0] = -1\n",
    "    n_shape[1] = num_sugars\n",
    "    a_shape[0] = -1\n",
    "    a_shape[1] = num_sugars\n",
    "    x_n_array = x_n_array.reshape(n_shape)\n",
    "    x_a_array = x_a_array.reshape(a_shape)\n",
    "\n",
    "    return x_array, y_array, x_n_array, x_a_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "# MODEL TRAINING\n",
    "# (if testing, skip this section and go straight to MODEL TESTING below)\n",
    "# (if denovo sequencing, skip this section and go straight to MODEL DENOVO SEQUENCING below)\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     4,
     10,
     20,
     42
    ]
   },
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "\n",
    "# prepare training_samples and glycan_y_superset\n",
    "# since they are too large, we save/load them from hard disk to reuse\n",
    "if os.path.exists(\"training_arrays/x_training.npy\"):\n",
    "    print(\"Load training data\")\n",
    "    x_training = np.load(\"training_arrays/x_training.npy\")\n",
    "    y_training = np.load(\"training_arrays/y_training.npy\")\n",
    "    x_n_training = np.load(\"training_arrays/x_n_training.npy\")\n",
    "    x_a_training = np.load(\"training_arrays/x_a_training.npy\")\n",
    "else:\n",
    "    print(\"Prepare training samples\")\n",
    "    fraction_id_list = [2, 3, 4, 5]\n",
    "    training_samples = prepare_training_samples(input_spectrum_file, glycan_psm, fraction_id_list)\n",
    "    print(\"len(training_samples) = \", len(training_samples))\n",
    "    glycan_y_superset = [w for x in training_samples for _,y,_ in x for z,_ in y for w in z]\n",
    "    print(\"len(glycan_y_superset) = \", len(glycan_y_superset))\n",
    "    glycan_y_superset = set(glycan_y_superset)\n",
    "    glycan_y_superset = sorted(list(glycan_y_superset))\n",
    "    print(\"len(glycan_y_superset) = \", len(glycan_y_superset))\n",
    "    with open(\"training_arrays/glycan_y_superset.pkl\", 'wb') as f:\n",
    "        pickle.dump(glycan_y_superset, f)\n",
    "    print()\n",
    "\n",
    "    # convert training_samples into np arrays\n",
    "    # since they are too large, we save/load them from hard disk to reuse\n",
    "    x_training, y_training, x_n_training, x_a_training = prepare_np_arrays(training_samples, glycan_y_superset)\n",
    "    np.save(\"training_arrays/x_training.npy\", x_training)\n",
    "    np.save(\"training_arrays/y_training.npy\", y_training)\n",
    "    np.save(\"training_arrays/x_n_training.npy\", x_n_training)\n",
    "    np.save(\"training_arrays/x_a_training.npy\", x_a_training)\n",
    "\n",
    "# check the shapes of the arrays\n",
    "print(\"x_training.shape = \", x_training.shape) # shape (num_samples*num_leaves, num_sugars*x_len)\n",
    "print(\"y_training.shape = \", y_training.shape) # shape (num_samples*num_leaves,)\n",
    "print(\"x_training[0] = \", x_training[0])\n",
    "print(\"y_training[0] = \", y_training[0])\n",
    "print(\"x_n_training.shape = \", x_n_training.shape) #  (num_samples*num_leaves, num_sugars, n_max, n_node_features)\n",
    "print(\"x_a_training.shape = \", x_a_training.shape) #  (num_samples*num_leaves, num_sugars, n_max, n_max)\n",
    "print()\n",
    "\n",
    "# train_test_split and normalization\n",
    "x_train, x_test, x_n_train, x_n_test, x_a_train, x_a_test, y_train, y_test = train_test_split(\n",
    "    x_training, \n",
    "    x_n_training, \n",
    "    x_a_training, \n",
    "    y_training, test_size=0.1, random_state=99)\n",
    "x_training_train_mean = np.mean(x_train, axis=0)\n",
    "# #x_training_train_std = np.std(x_train, axis=0) # the array is sparse, too many 0\n",
    "np.save(\"training_arrays/x_training_train_mean.npy\", x_training_train_mean)\n",
    "x_train_norm = (x_train - x_training_train_mean)\n",
    "x_test_norm = (x_test - x_training_train_mean)\n",
    "print(\"x_train_norm.shape = \", x_train.shape)\n",
    "print(\"x_test_norm.shape = \", x_test.shape)\n",
    "print(\"x_n_train.shape = \", x_n_train.shape)\n",
    "print(\"x_n_test.shape = \", x_n_test.shape)\n",
    "print(\"x_a_train.shape = \", x_a_train.shape)\n",
    "print(\"x_a_test.shape = \", x_a_test.shape)\n",
    "print()\n",
    "\n",
    "# transpose and group x, x_n, x_a into x_inputs, which is a list of 9 input arrays\n",
    "x_n_train, x_n_test, x_a_train, x_a_test = [np.transpose(x, axes=[1,0,2,3]) for x in [x_n_train, x_n_test, x_a_train, x_a_test]]\n",
    "print(\"x_train_norm.shape = \", x_train.shape)\n",
    "print(\"x_test_norm.shape = \", x_test.shape)\n",
    "print(\"x_n_train.shape = \", x_n_train.shape)\n",
    "print(\"x_n_test.shape = \", x_n_test.shape)\n",
    "print(\"x_a_train.shape = \", x_a_train.shape)\n",
    "print(\"x_a_test.shape = \", x_a_test.shape)\n",
    "x_inputs_train = [x_train_norm] + [(n, a) for n, a in zip(x_n_train, x_a_train)]\n",
    "x_inputs_test = [x_test_norm] + [(n, a) for n, a in zip(x_n_test, x_a_test)]\n",
    "print(\"len(x_inputs_train) = \", len(x_inputs_train))\n",
    "print(\"len(x_inputs_test) = \", len(x_inputs_test))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# delete training samples to free up memory\n",
    "# they are no longer needed after train_test_split\n",
    "del x_training, x_n_training, x_a_training, y_training\n",
    "del training_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     12
    ]
   },
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "\n",
    "# create model\n",
    "version = 'gnn'\n",
    "model = GnnModel(n_hidden=16, version=version)\n",
    "model._name = version + '_model'\n",
    "checkpoint_path = \"model_copy/cp.ckpt\"\n",
    "print(\"checkpoint_path =\", checkpoint_path)\n",
    "# train model\n",
    "loss_func = SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='Adam', loss=loss_func, metrics=['accuracy'])\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "history = model.fit(x_inputs_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[model_checkpoint],\n",
    "                    verbose=0,\n",
    "                   )\n",
    "\n",
    "# summary of model training\n",
    "print(model.summary())\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "epochs = range(1, len(loss)+1)\n",
    "print(\"min(loss) = {:.2f}\".format(min(loss)))\n",
    "print(\"min(val_loss) = {:.2f}\".format(min(val_loss)))\n",
    "print(\"max(accuracy) = {:.2f}\".format(max(accuracy)))\n",
    "print(\"max(val_accuracy) = {:.2f}\".format(max(val_accuracy)))\n",
    "\n",
    "# plot train/valid loss and accuracy\n",
    "# pyplot.subplot(1, 2, 1)\n",
    "# pyplot.plot(epochs, loss, label='loss')\n",
    "# pyplot.plot(epochs, val_loss, label='val_loss')\n",
    "# pyplot.xlabel('Epochs')\n",
    "# pyplot.ylabel('Loss')\n",
    "# pyplot.legend()\n",
    "\n",
    "# pyplot.subplot(1, 2, 2)\n",
    "# pyplot.plot(epochs, accuracy, label='accuracy')\n",
    "# pyplot.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "# pyplot.xlabel('Epochs')\n",
    "# pyplot.ylabel('Loss')\n",
    "# pyplot.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# delete training data to free up memory\n",
    "# they are no longer needed after training\n",
    "del x_train, x_test, x_n_train, x_n_test, x_a_train, x_a_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# MODEL TESTING\n",
    "# (if denovo sequencing, skip this section and go straight to MODEL DENOVO SEQUENCING below)\n",
    "######################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     4,
     10
    ]
   },
   "outputs": [],
   "source": [
    "# Prepare testing data\n",
    "\n",
    "# prepare testing_samples and load glycan_y_superset\n",
    "# since they are too large, we save/load them from hard disk to reuse\n",
    "if os.path.exists(\"training_arrays/x_testing.npy\"):\n",
    "    print(\"Load testing data\")\n",
    "    x_testing = np.load(\"training_arrays/x_testing.npy\")\n",
    "    y_testing = np.load(\"training_arrays/y_testing.npy\")\n",
    "    x_n_testing = np.load(\"training_arrays/x_n_testing.npy\")\n",
    "    x_a_testing = np.load(\"training_arrays/x_a_testing.npy\")\n",
    "else:\n",
    "    print(\"Prepare testing samples\")\n",
    "    fraction_id_list = [1]\n",
    "    testing_samples = prepare_training_samples(input_spectrum_file, glycan_psm, fraction_id_list)\n",
    "    print(\"len(testing_samples) = \", len(testing_samples))\n",
    "    with open(\"training_arrays/glycan_y_superset.pkl\", 'rb') as f:\n",
    "        glycan_y_superset = pickle.load(f)\n",
    "    print(\"len(glycan_y_superset) = \", len(glycan_y_superset))\n",
    "    print()\n",
    "\n",
    "    # convert testing_samples into np arrays\n",
    "    # since they are too large, we save/load them from hard disk to reuse\n",
    "    x_testing, y_testing, x_n_testing, x_a_testing = prepare_np_arrays(testing_samples, glycan_y_superset)\n",
    "    np.save(\"training_arrays/x_testing.npy\", x_testing)\n",
    "    np.save(\"training_arrays/y_testing.npy\", y_testing)\n",
    "    np.save(\"training_arrays/x_n_testing.npy\", x_n_testing)\n",
    "    np.save(\"training_arrays/x_a_testing.npy\", x_a_testing)\n",
    "\n",
    "# check the shapes of the arrays\n",
    "print(\"x_testing.shape = \", x_testing.shape)\n",
    "print(\"y_testing.shape = \", y_testing.shape)\n",
    "print(\"x_n_testing.shape = \", x_n_testing.shape)\n",
    "print(\"x_a_testing.shape = \", x_a_testing.shape)\n",
    "print()\n",
    "\n",
    "# normalization\n",
    "x_training_train_mean = np.load(\"training_arrays/x_training_train_mean.npy\")\n",
    "x_testing_norm = (x_testing - x_training_train_mean)\n",
    "print(\"x_testing_norm.shape = \", x_testing_norm.shape)\n",
    "print()\n",
    "\n",
    "# transpose and group x, x_n, x_a into x_inputs, which is a list of 9 input arrays\n",
    "x_n_testing, x_a_testing = [np.transpose(x, axes=[1,0,2,3]) for x in [x_n_testing, x_a_testing]]\n",
    "print(\"x_testing_norm.shape = \", x_testing_norm.shape)\n",
    "print(\"x_n_testing.shape = \", x_n_testing.shape)\n",
    "print(\"x_a_testing.shape = \", x_a_testing.shape)\n",
    "x_inputs_testing = [x_testing_norm] + [(n, a) for n, a in zip(x_n_testing, x_a_testing)]\n",
    "print(\"len(x_inputs_testing) = \", len(x_inputs_testing))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create, load and test model\n",
    "\n",
    "# create, compile  and load model\n",
    "version = 'gnn'\n",
    "model = GnnModel(n_hidden=16, version=version)\n",
    "model._name = version + '_model'\n",
    "checkpoint_path = \"model_\" + version + \"/cp.ckpt\"\n",
    "print(\"checkpoint_path =\", checkpoint_path)\n",
    "loss_func = SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='Adam', loss=loss_func, metrics=['accuracy'])\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "# test model\n",
    "testing_loss, testing_accuracy = model.evaluate(x_inputs_testing, y_testing)\n",
    "print('testing_loss = {:.2f}'.format(testing_loss))\n",
    "print('testing_accuracy = {:.2f}'.format(testing_accuracy))\n",
    "# double-check test set performance: accuracy (sensitivity/specificity/precision not applicable)\n",
    "y_prob = np.array(model.predict(x_inputs_testing))\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "y_correct = y_testing == y_pred\n",
    "print('double-check accuracy = {:.2f}'.format(np.sum(y_correct) / len(y_testing)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# MODEL DENOVO SEQUENCING\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2,
     42,
     43,
     47,
     91,
     196,
     203,
     205
    ]
   },
   "outputs": [],
   "source": [
    "# Predict de novo glycans with the pretrained model\n",
    "\n",
    "def gnn_denovo(mode, input_spectrum_file, glycan_psm, fraction_id_list,\n",
    "               version, checkpoint_path, x_training_train_mean_path, glycan_y_superset_path):\n",
    "    \n",
    "    print(\"gnn_denovo()\")\n",
    "\n",
    "    # create, compile  and load model\n",
    "    model = GnnModel(n_hidden=16, version=version)\n",
    "    model._name = version + '_model'\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "    # load x_train_mean of the training data for normalization\n",
    "    x_training_train_mean = np.load(x_training_train_mean_path)\n",
    "\n",
    "    # load glycan_y_superset of the training data\n",
    "    input_spectrum_handle = open(input_spectrum_file, 'r')\n",
    "    with open(glycan_y_superset_path, 'rb') as f:\n",
    "        glycan_y_superset = pickle.load(f)\n",
    "\n",
    "    # denvo sequencing parameters\n",
    "    target_glycan_ids = []\n",
    "    target_glycans = []\n",
    "    predict_glycans = []\n",
    "    beam_size = 1\n",
    "    print(\"beam_size =\", beam_size)\n",
    "    delta_mass_tolerance = 1.\n",
    "    delta_mass_left = 100.\n",
    "    # use a pseudo graph to control padding\n",
    "    pseudo_x = np.random.rand(18, 8)\n",
    "    pseudo_a = np.random.randint(0, 2, (18, 18))\n",
    "    pseudo_y = 0\n",
    "    pseudo_graph = None #Graph(x=pseudo_x, a=pseudo_a, y=pseudo_y)\n",
    "\n",
    "\n",
    "    for fraction_id in fraction_id_list:\n",
    "        psm_list = glycan_psm[fraction_id]\n",
    "        \n",
    "#         # unseen test\n",
    "#         psm_list = [x for x in psm_list if x['Glycan'] in unseen_glycan_composition]\n",
    "        \n",
    "        print(\"fraction_id = {0:d}, len(psm_list) = {1:d}\".format(fraction_id, len(psm_list)))\n",
    "        for index, psm in enumerate(psm_list[:]):\n",
    "            if ((index+1) % 100 == 0):\n",
    "                print(\"Processed {:d} PSMs...\".format(index+1))\n",
    "\n",
    "            # read peptide and glycan\n",
    "            if mode == 'evaluation':\n",
    "                peptide = psm['Peptide']\n",
    "                peptide_mass = float(psm['Mass'])\n",
    "                target_glycan_id = psm['Glycan ID']\n",
    "                target_glycan_ids.append(target_glycan_id)\n",
    "                target_glycan = glycan_dict[target_glycan_id]['GLYCAN'].clone()\n",
    "                target_glycans.append(target_glycan)\n",
    "                target_glycan_mass = float(psm['Glycan Mass'])\n",
    "                peptide_only_mass = peptide_mass - target_glycan_mass\n",
    "            elif mode == 'prediction':\n",
    "                # only do prediction on PSM with empty Glycan ID\n",
    "                if psm['Glycan ID']:\n",
    "                    predict_glycans.append([])\n",
    "                    continue\n",
    "                target_glycan_mass = float(psm['Glycan Mass'])\n",
    "                peptide_only_mass = float(psm['PepMass'])\n",
    "            \n",
    "            # note that glypy adds reducing end to the glycan mass, it's 18.01 Da larger than PEAKS reported mass\n",
    "            #print(target_glycan_mass)\n",
    "            #print(target_glycan.mass())\n",
    "            target_glycan_mass += 18.012115\n",
    "\n",
    "            # read spectrum\n",
    "            scan = 'F' + str(fraction_id) + ':' + psm['Scan']\n",
    "            mz1_list, intensity_list = get_spectrum(input_spectrum_handle, spectrum_location_dict, scan)\n",
    "\n",
    "\n",
    "            # find next candidate by iteratively adding 1 node to each leaf of each current candidate glycan\n",
    "            final_candidates = []\n",
    "            final_scores = []\n",
    "            x_len = len(glycan_y_superset)\n",
    "            for core_glycan in [n_link_core_fuc.clone(), n_link_core.clone()]:\n",
    "                core_len = len(core_glycan.index)\n",
    "                current_candidates = [core_glycan]\n",
    "                while current_candidates:\n",
    "                    # contruct next_candidates glycans \n",
    "                    next_candidates = []\n",
    "                    x_array = []\n",
    "                    for glycan in current_candidates:\n",
    "                        glycan.reindex(method='bfs')\n",
    "                        # core leaves\n",
    "                        leaves = [x for x in glycan.index[core_len-2:core_len] if len(x.children()) < 1]\n",
    "                        # branch leaves\n",
    "                        leaves += [x for x in glycan.index[core_len:] if len(x.children()) < 1]\n",
    "                        for leaf in leaves[:]:\n",
    "        #                     for glycoct in sugar_dict:\n",
    "        #                         sugar = sugar_dict[glycoct].clone()\n",
    "                            x_subarray = np.zeros((num_sugars, x_len))\n",
    "                            for candidate, name in enumerate(sugar_classes):\n",
    "                                sugar = glypy.monosaccharides[name].clone()\n",
    "                                leaf.add_monosaccharide(sugar)\n",
    "        #                         glycan.reindex()\n",
    "        #                         glycan.canonicalize()\n",
    "                                glycan_y_list, glycopsm = compute_glycopsm_score(glycan, peptide_only_mass, mz1_list, intensity_list)\n",
    "                                for y, score in zip(glycan_y_list, glycopsm):\n",
    "                                    if y in glycan_y_superset:\n",
    "                                        idx = glycan_y_superset.index(y)\n",
    "                                        x_subarray[candidate, idx] = score\n",
    "                                next_candidates.append(glycan.clone())\n",
    "                                sugar.drop_monosaccharide(-1)\n",
    "                            x_array.append(x_subarray)\n",
    "\n",
    "                    # extract input features for glycan_y_superset model # (candidates * leaves, num_sugars*x_len)\n",
    "                    x_array = np.array(x_array) # (candidates * leaves, num_sugars, x_len)\n",
    "                    x_array = np.reshape(x_array, (-1, num_sugars*x_len))\n",
    "                    x_array_norm = (x_array - x_training_train_mean)\n",
    "\n",
    "                    # extract input trees for GNN model\n",
    "                    x_trees_labels = [(tree, 0) for tree in next_candidates]\n",
    "                    # convert trees to graphs\n",
    "                    x_graphs = Trees_to_Graphs(x_trees_labels, pseudo_graph) # shape (1 + num_samples*num_leaves*num_sugars,)\n",
    "                    # use BatchLoader to do zero padding for all graphs to have the same size\n",
    "                    # put all graphs into 1 batch\n",
    "                    loader = BatchLoader(x_graphs, batch_size=len(x_graphs), shuffle=False)\n",
    "                    x_n_array = []\n",
    "                    x_a_array = []\n",
    "                    for batch in loader.load():\n",
    "                        # each batch is a tuple (inputs, labels)\n",
    "                        # inputs is a tuple containing:\n",
    "                        # 0: node attributes of shape [batch, n_max, n_node_features];\n",
    "                        # 1: adjacency matrices of shape [batch, n_max, n_max];\n",
    "                        x_n_array.append(batch[0][0]) # node features\n",
    "                        x_a_array.append(batch[0][1]) # adjacency matrix\n",
    "                        # because batch_size=len(x_graphs), one iteration is enough\n",
    "                        break\n",
    "                    x_n_array = np.array(x_n_array) # shape (1, 1 + num_samples*num_leaves*num_sugars, n_max, n_node_features)\n",
    "                    x_a_array = np.array(x_a_array) # shape (1, 1 + num_samples*num_leaves*num_sugars, n_max, n_max)\n",
    "                    # exclude pseudo_graph if not None\n",
    "                    if pseudo_graph:\n",
    "                        x_n_array = x_n_array[:,1:,:,:]\n",
    "                        x_a_array = x_a_array[:,1:,:,:]\n",
    "                    # reshape to (num_samples*num_leaves, num_sugars, , )\n",
    "                    n_shape = list(x_n_array.shape)\n",
    "                    a_shape = list(x_a_array.shape)\n",
    "                    n_shape[0] = -1\n",
    "                    n_shape[1] = num_sugars\n",
    "                    a_shape[0] = -1\n",
    "                    a_shape[1] = num_sugars\n",
    "                    x_n_array = x_n_array.reshape(n_shape)\n",
    "                    x_a_array = x_a_array.reshape(a_shape)\n",
    "\n",
    "                    # combine inputs\n",
    "                    x_n_array, x_a_array = [np.transpose(x, axes=[1,0,2,3]) for x in [x_n_array, x_a_array]]\n",
    "                    x_inputs = [x_array_norm] + [(n, a) for n, a in zip(x_n_array, x_a_array)]\n",
    "\n",
    "                    # predict scores for next_candidates, sort and keep top-10\n",
    "                    next_scores = model.predict(x_inputs) # (candidates * leaves, num_sugars)\n",
    "\n",
    "                    # only allow 1 sugar with max score per leaf\n",
    "        #             next_candidates_split = [next_candidates[i:i + num_sugars] for i in range(0, len(next_candidates), num_sugars)]\n",
    "        #             next_scores_argmax = np.argmax(next_scores, axis=1)\n",
    "        #             next_candidates = [x[y] for x, y in zip(next_candidates_split, next_scores_argmax)]\n",
    "        #             next_scores = np.amax(next_scores, axis=1)\n",
    "                    # allow all sugars per leaf\n",
    "                    next_scores = next_scores.flatten() # (candidates * leaves * num_sugars)\n",
    "\n",
    "                    # check candidates for mass, sort and keep top-10\n",
    "                    current_candidates = []\n",
    "                    current_scores = []\n",
    "                    for glycan, score in zip(next_candidates, next_scores):\n",
    "                        if abs(target_glycan_mass - glycan.mass()) <= delta_mass_tolerance:\n",
    "                            final_candidates.append(glycan.clone())\n",
    "                            final_scores.append(score)\n",
    "                        elif glycan.mass() < (target_glycan_mass - delta_mass_left):\n",
    "                            current_candidates.append(glycan.clone())\n",
    "                            current_scores.append(score)\n",
    "                    if current_candidates:\n",
    "                        current_sorted = sorted(zip(current_candidates, current_scores), key=lambda pair: -pair[1])\n",
    "                        current_candidates = [x for x, y in current_sorted[:beam_size]]\n",
    "\n",
    "            # sort final candidates\n",
    "            if final_candidates:\n",
    "                final_sorted = sorted(zip(final_candidates, final_scores), key=lambda pair: -pair[1])\n",
    "                predict_glycans.append(final_sorted)\n",
    "            else:\n",
    "                predict_glycans.append([])\n",
    "        \n",
    "    input_spectrum_handle.close()\n",
    "    return target_glycans, predict_glycans\n",
    "\n",
    "# input_spectrum_file: already defined in I/O FUNCTIONS and DATA PRE-PROCESSING\n",
    "# glycan_psm: already defined in I/O FUNCTIONS and DATA PRE-PROCESSING\n",
    "fraction_id_list = [1, 2, 3]\n",
    "version = 'gnn'\n",
    "checkpoint_path = \"model_\" + version + \"/cp.ckpt\"\n",
    "print(\"checkpoint_path =\", checkpoint_path)\n",
    "x_training_train_mean_path = \"training_arrays/x_training_train_mean.npy\"\n",
    "glycan_y_superset_path = \"training_arrays/glycan_y_superset.pkl\"\n",
    "basic_denovo_time = time.time()\n",
    "target_glycans, predict_glycans = gnn_denovo(\n",
    "    mode,\n",
    "    input_spectrum_file, glycan_psm, fraction_id_list,\n",
    "    version, checkpoint_path, x_training_train_mean_path, glycan_y_superset_path)\n",
    "basic_denovo_time = time.time() - basic_denovo_time\n",
    "print(\"basic_denovo_time =\", basic_denovo_time)\n",
    "\n",
    "if mode == 'evaluation':\n",
    "    test_glycan_accuracy(target_glycans[:1000], predict_glycans[:1000], top=1)\n",
    "elif mode == 'prediction':\n",
    "    fieldnames = list(glycan_psm[1][0].keys())\n",
    "    fieldnames += ['denovo glycan ID', 'denovo glycan score', 'denovo glycan composition']\n",
    "    with open(data_folder + 'denovo_glycan.csv', 'w', newline='') as csvfile:\n",
    "        with open(data_folder + 'denovo_glycan.txt', 'w') as txtfile:\n",
    "            csvwriter = csv.DictWriter(csvfile, fieldnames)\n",
    "            csvwriter.writeheader()\n",
    "            psm_list_flatten = [psm for x in fraction_id_list for psm in glycan_psm[x]]\n",
    "            num_predict = 0\n",
    "            unique_compositions = set()\n",
    "            for psm, candidates in zip(psm_list_flatten, predict_glycans):\n",
    "                if candidates:\n",
    "                    num_predict += 1\n",
    "                    glycan, score = candidates[0]\n",
    "                    row = psm.copy()\n",
    "                    composition = str(glypy.structure.glycan_composition.GlycanComposition.from_glycan(glycan))\n",
    "                    if composition not in unique_compositions:\n",
    "                        unique_compositions.add(composition)\n",
    "                    # output to csv file\n",
    "                    row['denovo glycan ID'] = num_predict\n",
    "                    row['denovo glycan score'] = score\n",
    "                    row['denovo glycan composition'] = composition\n",
    "                    csvwriter.writerow(row)\n",
    "                    # output to txt file\n",
    "                    lines = ['GLYCAN START\\n']\n",
    "                    lines.append('GLYCANID={0}\\n'.format(num_predict))\n",
    "                    lines.append('GLYCAN_MASS={0}\\n'.format(glycan.mass() - 18.012115))\n",
    "                    lines.append('GLYCAN_TAXON=Others\\n')\n",
    "                    lines.append(str(glycan))\n",
    "                    lines.append('GLYCAN END\\n')\n",
    "                    lines.append('\\n')\n",
    "                    txtfile.write(''.join(lines))\n",
    "        print(\"len(psm_list_flatten) =\", len(psm_list_flatten))\n",
    "        print(\"num_predict =\", num_predict)\n",
    "        print(\"len(unique_compositions) =\", len(unique_compositions))\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
